# Три классических метода оптимизации

## Тестирование

Во всех случаях использовали тестовую функцию вида:

$$
f(x, y) = - e^{-x^2 - y^2}
$$

## Метод оптимизации Нелдера — Мида

Алгоритм заключается в формировании симплекса и последующего его деформирования в направлении минимума, посредством трех операций:

Логика работы алгоритма полностью иллюстрируется на этой чудной картинке:

![image](https://github.com/timattt/Computational-math/assets/25401699/42770efd-cdff-42aa-934c-58eff44c1da8)

### Результаты работы

Можем пронаблюдать несколько итераций симплекса.

![image](https://github.com/timattt/Computational-math/assets/25401699/81c00072-c4fb-4374-bc88-d10b1af020e3)

## Метод BFGS

Метод второго порядка, который использует разложение функции в полином второй степени.

$$
f(x_k + p) = f(x_k) + \nabla f^T(x_k)p + \frac{1}{2}p^TH(x_k)p
$$

### Алгоритм

* Итерируемся до тех пор, пока модуль градиента больше требуемой точности
* Начальное приближение гессиана может быть единичной матрицей
* Находим направление:

$$
p_k = -C_k \nabla f_k
$$

* Вычисляем $$x_{k+1} = x_k + \alpha_k + p_k$$

где $\alpha_k$ удовлетворяет условиям Вольфе:

$$
f(x_k + \alpha_k p_k) \leq f(x_k) + c_1 \alpha_k \nabla f_k^T p_k
$$
и
$$
\nabla f(x_k + \alpha_k p_k)^T p_k \geq c_2 \nabla f_k(T) p_k
$$

Это можно найти с помощью обычного линейного поиска.

* Обозначаем $s_k = x_{k+1} - x_k$ и $y_k = \nabla f_{k+1} - \nabla f_k$
* Вычисляем новый приблизительный гессиан (а точнее его обратную матрицу, ибо так проще): 

$$
C_{k+1} = (I - \rho_k s_k y_k^T)C_k(I - \rho_k y_k s_k^T) + \rho_k s_k s_k^T
$$

где

$$
\rho_k = \frac{1}{y_k^T s_k}
$$

### Результаты работы

![image](https://github.com/timattt/Computational-math/assets/25401699/32301993-7114-4753-9480-2f2abc71f937)

## Метод доверительной области

Смысла метода в том, что мы на каждой итерации решаем квадратичную задачу в какой-то доверительной области, где считаем, что квадратичная апроксимация будет уместна.

$$
min_{p \in R^n} f_k + p^T g_k + \frac{1}{2}p^T B_k p
$$

s.t. $|p| < \Delta_k$

Последнее здесь - это trust region redius.

Чтобы на каждом шаге находить p, можно воспользоваться методом DOGLEG.

### Результаты работы

![image](https://github.com/timattt/Computational-math/assets/25401699/8bfeda78-a4ec-454d-bb46-a53021a18438)

## Сравнение алгоритмов

Получим зависимость количества итераций от точности для всех трех алгоритмов.

![image](https://github.com/timattt/Computational-math/assets/25401699/8bf3db5d-5bad-4be5-b1f1-cec709ffbe69)
